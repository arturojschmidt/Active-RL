



Goal : determine optimal policy at each state

7 states


For each state, we are given the probability distribution of its next states.

Don't use these to determine the probability, but to choose and action randomly, reinforcement learning should then predict these probabilities

NOTE -- we have enough information to solve as MDP but don't


Since we are implementing Active Learning -- use diff values for exploration vs exploitation


Active reinforcement learning takes actions a, calculates probability of state s' after trials? Learns utility of each state to create a policy?


Model-based:
	try to learn the model while acting in environment
	track number of times state s' follow state s action a
	update transition probability p(s'|s,a)
	estimate utility using bellman equations
	U(s) = R(s)+ γ max ∑ P(s’|s,a)U(s’)
		           s'
	choose action that maximizes expected utility
	pi(s) = argmax ∑ P(s’|s,a)U(s’)

	
	
